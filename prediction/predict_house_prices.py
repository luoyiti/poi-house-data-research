"""
æˆ¿ä»·é¢„æµ‹å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼° - ä¼˜åŒ–ç‰ˆï¼ˆæ— é›†æˆï¼‰
ç›®æ ‡ï¼šRÂ² > 0.7
"""

import pandas as pd
import numpy as np
import os
import warnings
from datetime import datetime
import pickle
import json
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
import xgboost as xgb
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 12
plt.rcParams['figure.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10
sns.set_style("whitegrid")

warnings.filterwarnings('ignore')

# ========================= é…ç½® =========================
DATA_FILE = r"D:\HuaweiMoveData\Users\32549\OneDrive\å¤§æ•°æ®å°ç»„ä½œä¸š\final_house_features.csv"
MODEL_DIR = os.path.join(os.path.dirname(DATA_FILE), 'models')
os.makedirs(MODEL_DIR, exist_ok=True)
RANDOM_STATE = 42

# ========================= åŠ è½½æ•°æ® =========================
df = pd.read_csv(DATA_FILE)
print(f" æ•°æ®åŠ è½½æˆåŠŸï¼Œæ ·æœ¬æ•°: {len(df):,}, ç‰¹å¾æ•°: {len(df.columns)}")

# ========================= æ•°æ®é¢„å¤„ç† =========================
def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†"""
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
    print(df.isnull().sum())
    
    # ç§»é™¤å¯èƒ½çš„å¼‚å¸¸å€¼ (ä»·æ ¼è¿‡é«˜æˆ–è¿‡ä½çš„æç«¯å€¼)
    price_col = 'price_per_meter'
    Q1 = df[price_col].quantile(0.05)
    Q3 = df[price_col].quantile(0.95)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    original_len = len(df)
    df = df[(df[price_col] >= lower_bound) & (df[price_col] <= upper_bound)]
    print(f"ç§»é™¤å¼‚å¸¸å€¼: {original_len - len(df)} ä¸ªæ ·æœ¬")
    
    return df

df = preprocess_data(df)

# ========================= ç‰¹å¾å·¥ç¨‹ =========================
def create_features(df):
    """åˆ›å»ºæ–°ç‰¹å¾"""
    # äº¤äº’ç‰¹å¾
    df['size_poi_interaction'] = df['Size_num'] * df['poi_total_score']
    df['bedroom_poi_interaction'] = df['Bedroom'] * df['poi_total_score']
    
    # ç»¼åˆè¯„åˆ†ç‰¹å¾
    df['overall_quality_score'] = (
        df['Building_Hardware_Score'] + 
        df['Structure_Score'] + 
        df['Elevator_Floor_Score'] + 
        df['Orientation_Score']
    ) / 4
    
    # æˆ¿é—´æ€»æ•°
    df['total_rooms'] = df['Bedroom'] + df['Washroom'] + df['Livingroom'] + df['Kitchen']
    
    # é¢ç§¯ä¸æˆ¿é—´æ¯”ä¾‹
    df['size_per_room'] = df['Size_num'] / df['total_rooms'].replace(0, 1)
    
    return df

df = create_features(df)

# ========================= ç‰¹å¾ä¸ç›®æ ‡ =========================
target = 'price_per_meter'

# æ‰©å±•ç‰¹å¾é›†
feature_cols = [
    'Building_Hardware_Score',
    'Structure_Score', 
    'Elevator_Floor_Score',
    'Orientation_Score',
    'StairsUnitRatio_Score',
    'Size_num',
    'Bedroom',
    'Washroom', 
    'Livingroom',
    'Kitchen',
    'poi_total_score',
    'size_poi_interaction',
    'bedroom_poi_interaction',
    'overall_quality_score',
    'total_rooms',
    'size_per_room'
]

# ç§»é™¤å¯èƒ½åŒ…å«NaNçš„ç‰¹å¾
feature_cols = [col for col in feature_cols if col in df.columns and not df[col].isnull().any()]

print(f"æœ€ç»ˆä½¿ç”¨ç‰¹å¾æ•°: {len(feature_cols)}")
print("ç‰¹å¾åˆ—è¡¨:", feature_cols)

X = df[feature_cols]
y = df[target]

# ========================= æ•°æ®é›†åˆ’åˆ† =========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=pd.qcut(y, q=5, duplicates='drop')
)
print(f"è®­ç»ƒé›†: {len(X_train):,} æ ·æœ¬, æµ‹è¯•é›†: {len(X_test):,} æ ·æœ¬")

# ========================= ç‰¹å¾ç¼©æ”¾ =========================
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ========================= ä¼˜åŒ–æ¨¡å‹å®šä¹‰ =========================
models = {
    'Ridge': Ridge(random_state=RANDOM_STATE),
    'Lasso': Lasso(random_state=RANDOM_STATE),
    'ElasticNet': ElasticNet(random_state=RANDOM_STATE),
    'RandomForest': RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1),
    'GradientBoosting': GradientBoostingRegressor(random_state=RANDOM_STATE),
    'XGBoost': xgb.XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1),
    'SVR': SVR(),
    'MLP': MLPRegressor(random_state=RANDOM_STATE, max_iter=1000)
}

# ========================= è¶…å‚æ•°è°ƒä¼˜ =========================
param_grids = {
    'Ridge': {'alpha': [0.1, 1.0, 10.0, 100.0]},
    'Lasso': {'alpha': [0.001, 0.01, 0.1, 1.0]},
    'ElasticNet': {'alpha': [0.01, 0.1, 1.0], 'l1_ratio': [0.2, 0.5, 0.8]},
    'RandomForest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'GradientBoosting': {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.05, 0.1, 0.15],
        'max_depth': [3, 5, 7],
        'subsample': [0.8, 0.9, 1.0]
    },
    'XGBoost': {
        'n_estimators': [100, 200, 300],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.05, 0.1, 0.15],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0]
    },
    'LightGBM': {
        'n_estimators': [100, 200, 300],
        'max_depth': [6, 8, 10, -1],
        'learning_rate': [0.05, 0.1, 0.15],
        'num_leaves': [31, 63, 127],
        'subsample': [0.8, 0.9, 1.0]
    },
    'SVR': {
        'C': [0.1, 1.0, 10.0, 100.0],
        'kernel': ['rbf', 'linear'],
        'gamma': ['scale', 'auto']
    },
    'MLP': {
        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
        'alpha': [0.0001, 0.001, 0.01],
        'learning_rate': ['constant', 'adaptive'],
        'early_stopping': [True]
    }
}

# ========================= æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° =========================
def evaluate_regression(y_true, y_pred):
    """è®¡ç®—å›å½’æŒ‡æ ‡"""
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    return {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'MAPE': mape
    }

overall_results = {}
best_model = None
best_r2 = -np.inf
tuned_models = {}

print("\nå¼€å§‹æ¨¡å‹è°ƒä¼˜...")
for model_name, model in tqdm(models.items(), desc="è°ƒä¼˜æ¨¡å‹"):
    print(f"\nè°ƒä¼˜ {model_name} ...")
    
    try:
        if model_name in param_grids:
            # ä½¿ç”¨ç½‘æ ¼æœç´¢è°ƒä¼˜
            grid_search = GridSearchCV(
                model, param_grids[model_name], 
                cv=5, scoring='r2', n_jobs=-1, verbose=0
            )
            grid_search.fit(X_train_scaled, y_train)
            
            best_model_instance = grid_search.best_estimator_
            tuned_models[model_name] = best_model_instance
            print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        else:
            # ç›´æ¥è®­ç»ƒ
            model.fit(X_train_scaled, y_train)
            best_model_instance = model
            tuned_models[model_name] = best_model_instance
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_test = best_model_instance.predict(X_test_scaled)
        metrics = evaluate_regression(y_test, y_pred_test)
        overall_results[model_name] = metrics
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(best_model_instance, X_train_scaled, y_train, 
                                  cv=5, scoring='r2')
        
        print(f"  æµ‹è¯•é›† R2: {metrics['R2']:.4f}")
        print(f"  äº¤å‰éªŒè¯ R2: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        print(f"  RMSE: {metrics['RMSE']:.2f}, MAE: {metrics['MAE']:.2f}, MAPE: {metrics['MAPE']:.1f}%")
        
        if metrics['R2'] > best_r2:
            best_r2 = metrics['R2']
            best_model = (model_name, best_model_instance)
            
    except Exception as e:
        print(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        overall_results[model_name] = None

# ========================= ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š =========================
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
report_file = os.path.join(MODEL_DIR, f"optimized_regression_report_{timestamp}.txt")

with open(report_file, 'w', encoding='utf-8') as f:
    f.write("="*80 + "\n")
    f.write("æˆ¿ä»·é¢„æµ‹å¤šæ¨¡å‹è®­ç»ƒè¯„ä¼°æŠ¥å‘Š (ä¼˜åŒ–ç‰ˆ - æ— é›†æˆ)\n")
    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n")
    f.write("="*80 + "\n\n")
    
    f.write("ã€æ•°æ®æ¦‚å†µã€‘\n")
    f.write(f"æ€»æ ·æœ¬æ•°: {len(df):,}\n")
    f.write(f"ç‰¹å¾æ•°: {len(feature_cols)}\n")
    f.write(f"è®­ç»ƒé›†: {len(X_train):,}\n")
    f.write(f"æµ‹è¯•é›†: {len(X_test):,}\n\n")
    
    f.write("ã€æ¨¡å‹æ€§èƒ½å¯¹æ¯”ã€‘\n")
    f.write("-" * 70 + "\n")
    f.write(f"{'æ¨¡å‹':<20} {'R2':<10} {'RMSE':<12} {'MAE':<12} {'MAPE':<10}\n")
    f.write("-" * 70 + "\n")
    
    for model_name, metrics in overall_results.items():
        if metrics:
            f.write(f"{model_name:<20} {metrics['R2']:.4f}    {metrics['RMSE']:<10.2f} {metrics['MAE']:<10.2f} {metrics['MAPE']:<8.1f}%\n")
    
    f.write("-" * 70 + "\n\n")
    
    if best_model:
        f.write(f"ã€æœ€ä½³æ¨¡å‹ã€‘{best_model[0]} - R2={best_r2:.4f}\n")
        
        if best_r2 >= 0.7:
            f.write("ğŸ‰ ç›®æ ‡è¾¾æˆï¼RÂ² > 0.7\n")
        else:
            f.write("âš ï¸ æœªè¾¾åˆ°ç›®æ ‡ RÂ² > 0.7ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–\n")

print(f"\n æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

# ========================= å¯è§†åŒ–ç»“æœ =========================
fig = plt.figure(figsize=(16, 12))

# 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
ax1 = plt.subplot(2, 2, 1)
model_names = [name for name, metrics in overall_results.items() if metrics]
r2_scores = [metrics['R2'] for metrics in overall_results.values() if metrics]
colors = ['green' if r2 >= 0.7 else 'orange' for r2 in r2_scores]

bars = plt.bar(range(len(model_names)), r2_scores, color=colors, alpha=0.7)
plt.axhline(y=0.7, color='red', linestyle='--', linewidth=2, label='ç›®æ ‡çº¿ (RÂ²=0.7)')
plt.xticks(range(len(model_names)), model_names, rotation=45, ha='right')
plt.ylabel('RÂ² Score', fontsize=12)
plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)

# åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')

# 2. ç‰¹å¾é‡è¦æ€§
if best_model and hasattr(best_model[1], 'feature_importances_'):
    ax2 = plt.subplot(2, 2, 2)
    fi = pd.DataFrame({
        'feature': feature_cols,
        'importance': best_model[1].feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.barh(fi.head(10)['feature'], fi.head(10)['importance'], color='skyblue')
    plt.xlabel('Importance', fontsize=12)
    plt.title(f'Top 10 ç‰¹å¾é‡è¦æ€§ - {best_model[0]}', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    
    # ä¸ºç‰¹å¾é‡è¦æ€§æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (importance, feature) in enumerate(zip(fi.head(10)['importance'], fi.head(10)['feature'])):
        plt.text(importance, i, f' {importance:.3f}', va='center', fontsize=9)

# 3. é¢„æµ‹ vs å®é™…å€¼
ax3 = plt.subplot(2, 2, 3)
if best_model:
    y_pred_best = best_model[1].predict(X_test_scaled)
    plt.scatter(y_test, y_pred_best, alpha=0.6, color='steelblue')
    
    # æ·»åŠ ç†æƒ³çº¿
    max_val = max(y_test.max(), y_pred_best.max())
    min_val = min(y_test.min(), y_pred_best.min())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
    
    plt.xlabel('å®é™…ä»·æ ¼', fontsize=12)
    plt.ylabel('é¢„æµ‹ä»·æ ¼', fontsize=12)
    plt.title(f'é¢„æµ‹ vs å®é™…å€¼ (RÂ²={best_r2:.4f})', fontsize=14, fontweight='bold')

# 4. æ®‹å·®å›¾
ax4 = plt.subplot(2, 2, 4)
if best_model:
    residuals = y_test - y_pred_best
    plt.scatter(y_pred_best, residuals, alpha=0.6, color='coral')
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.xlabel('é¢„æµ‹ä»·æ ¼', fontsize=12)
    plt.ylabel('æ®‹å·®', fontsize=12)
    plt.title('æ®‹å·®åˆ†æ', fontsize=14, fontweight='bold')

plt.tight_layout()
plot_file = os.path.join(MODEL_DIR, f"optimized_results_{timestamp}.png")
plt.savefig(plot_file, dpi=150, bbox_inches='tight')
plt.show()
plt.close()

print(f"ç»“æœå›¾å·²ä¿å­˜: {plot_file}")

# ========================= ä¿å­˜æœ€ä½³æ¨¡å‹å’Œé¢„å¤„ç†å™¨ =========================
if best_model:
    model_name, model = best_model
    
    # ä¿å­˜æ¨¡å‹
    model_file = os.path.join(MODEL_DIR, f"best_model_{model_name}_{timestamp}.pkl")
    with open(model_file, 'wb') as f:
        pickle.dump({
            'model': model,
            'scaler': scaler,
            'feature_cols': feature_cols,
            'metrics': overall_results[model_name] if model_name in overall_results else None
        }, f)
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    y_pred_best = model.predict(X_test_scaled)
    pred_df = pd.DataFrame({
        'id': df.iloc[X_test.index]['id'].values if 'id' in df.columns else X_test.index,
        'y_true': y_test,
        'y_pred': y_pred_best,
        'residual': y_test - y_pred_best
    })
    pred_file = os.path.join(MODEL_DIR, f"predictions_{timestamp}.csv")
    pred_df.to_csv(pred_file, index=False, encoding='utf-8-sig')
    
    print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_file}")
    print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜: {pred_file}")
    
    # æœ€ç»ˆç»“æœæ±‡æ€»
    print("\n" + "="*60)
    print("æœ€ç»ˆç»“æœæ±‡æ€»")
    print("="*60)
    print(f"æœ€ä½³æ¨¡å‹: {best_model[0]}")
    print(f"æµ‹è¯•é›† RÂ²: {best_r2:.4f}")
   

print("\næ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")